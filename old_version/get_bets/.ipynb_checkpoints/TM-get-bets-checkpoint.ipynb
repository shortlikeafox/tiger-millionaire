{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based off of TM-Evaluate-Models-Worksheet in that instead of getting a score for a test set we are printing bets for \n",
    "#a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "remove_fight_island = False    #Removes fight island fights from consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../automated_model_creation') #We need to access the function file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import custom_cv_eval, get_ev_from_df, get_bet_ev, get_bet_return\n",
    "import random\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4759"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/kaggle_data/ufc-master.csv\")\n",
    "df_upcoming = pd.read_csv('../data/kaggle_data/upcoming-event.csv')\n",
    "len(df)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4759"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The fight island fights were messing with the established models.  This function most likely isn't needed anymore\n",
    "if remove_fight_island:\n",
    "    ##Let's remove the Fight island contests and see how that affects score\n",
    "    df_no_fight_island = df[(df['location'] != 'Abu Dhabi, Abu Dhabi, United Arab Emirates')]\n",
    "    #df_train_no_fight_island = df_train[(df_train['location'] != 'Abu Dhabi, Abu Dhabi, United Arab Emirates')]\n",
    "    df = df_no_fight_island\n",
    "    #df_train = df_train_no_fight_island\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_upcoming_fights = len(df_upcoming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4772"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_upcoming.append(df)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e6abcb5e0e70>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = 0\n",
      "<ipython-input-10-e6abcb5e0e70>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = 1\n"
     ]
    }
   ],
   "source": [
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "mask = df['Winner'] == 'Red'\n",
    "df['label'][mask] = 0\n",
    "mask = df['Winner'] == 'Blue'\n",
    "df['label'][mask] = 1\n",
    "\n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a label df:\n",
    "label_df = df['label']\n",
    "\n",
    "#Let's create an odds df too:\n",
    "odds_df = df[['R_odds', 'B_odds']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "13\n",
      "13\n",
      "4759\n",
      "4759\n",
      "4759\n"
     ]
    }
   ],
   "source": [
    "#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n",
    "#as we pick models\n",
    "\n",
    "df_train = df[num_upcoming_fights:]\n",
    "odds_train = odds_df[num_upcoming_fights:]\n",
    "label_train = label_df[num_upcoming_fights:]\n",
    "\n",
    "df_test = df[:num_upcoming_fights]\n",
    "odds_test = odds_df[:num_upcoming_fights]\n",
    "label_test = label_df[:num_upcoming_fights]\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(odds_test))\n",
    "print(len(label_test))\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(odds_train))\n",
    "print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_train)\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_bets(ev_df, min_ev):\n",
    "    \n",
    "    for index, row in ev_df.iterrows():\n",
    "        t1_bet_ev = get_bet_ev(row['t1_odds'], row['t1_prob'])\n",
    "        t2_bet_ev = get_bet_ev(row['t2_odds'], row['t2_prob'])\n",
    "        #print(f\"ODDS:{row['t2_odds']} PROB: {row['t2_prob']} EV: {t2_bet_ev}\")\n",
    "        t1_bet_return = get_bet_return(row['t1_odds'])\n",
    "        t2_bet_return = get_bet_return(row['t2_odds'])\n",
    "        print(f\"{row['t1_name']} vs. {row['t2_name']}\")\n",
    "        if t1_bet_ev > min_ev:\n",
    "            print(f\"{row['t1_name']} has an EV of {round(t1_bet_ev, 2)} on odds of {round(row['t1_odds'],2)}.  They have {round(row['t1_prob']*100,2)}\", \n",
    "                  \"% chance of winning.\")\n",
    "        \n",
    "        elif t2_bet_ev > min_ev:\n",
    "            print(f\"{row['t2_name']} has an EV of {round(t2_bet_ev, 2)} on odds of {round(row['t2_odds'],2)}.  They have {round(row['t2_prob']*100,2)}\", \n",
    "                  \"% chance of winning.\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"No bets.  EV1:{t1_bet_ev}. EV2: {t2_bet_ev}\")\n",
    "            print(f\"{row['t1_name']} has an EV of {round(t1_bet_ev, 2)} on odds of {round(row['t1_odds'],2)}.  They have {round(row['t1_prob']*100,2)}\", \n",
    "                  \"% chance of winning.\")\n",
    "        \n",
    "        print()\n",
    "        \n",
    "    return(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bets(input_model, input_features, input_ev, train_df, train_labels, train_odds, test_df, test_labels,\n",
    "             test_odds, verbose=True):\n",
    "    fighters_test = df[['R_fighter', 'B_fighter']]\n",
    "    #Train model.  \n",
    "    df_train = train_df[input_features].copy()\n",
    "    df_test = test_df[input_features].copy()\n",
    "    df_train = df_train.dropna()\n",
    "    df_test = df_test.dropna()\n",
    "        \n",
    "    df_train = pd.get_dummies(df_train)\n",
    "    df_test = pd.get_dummies(df_test)\n",
    "    df_train, df_test = df_train.align(df_test, join='left', axis=1)    #Ensures both sets are dummified the same\n",
    "    df_test = df_test.fillna(0)\n",
    "\n",
    "    #LOOK AT get_ev and prepare the labels and odds\n",
    "    \n",
    "    labels_train = train_labels[train_labels.index.isin(df_train.index)]\n",
    "    odds_train = train_odds[train_odds.index.isin(df_train.index)] \n",
    "    labels_test = test_labels[test_labels.index.isin(df_test.index)]\n",
    "    odds_test = test_odds[test_odds.index.isin(df_test.index)] \n",
    "    fighters_test = fighters_test[fighters_test.index.isin(df_test.index)]\n",
    "\n",
    "    input_model.fit(df_train, labels_train)\n",
    "    \n",
    "    probs = input_model.predict_proba(df_test)\n",
    "    odds_test = np.array(odds_test)  \n",
    "    fighters_test = np.array(fighters_test)\n",
    "    #display(fighters_test)\n",
    "    prepped_test = list(zip(fighters_test[:, -2], fighters_test[:, -1], odds_test[:, -2], odds_test[:, -1], \n",
    "                            probs[:, 0], probs[:, 1], labels_test))\n",
    "    ev_prepped_df = pd.DataFrame(prepped_test, columns=['t1_name', 't2_name', 't1_odds', 't2_odds', 't1_prob', 't2_prob', 'winner'])\n",
    "    \n",
    "    #display(ev_prepped_df)\n",
    "    \n",
    "    display_bets(ev_prepped_df, input_ev)\n",
    "    \n",
    "\n",
    "    return(ev_prepped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../automated_model_creation/models.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    models = list(reader)\n",
    "    \n",
    "#print(len(models))\n",
    "\n",
    "#set the model choice:\n",
    "model_choice = 7\n",
    "\n",
    "print(models)\n",
    "\n",
    "\n",
    "\n",
    "model = eval(models[1][model_choice])\n",
    "features = eval(models[2][model_choice])\n",
    "ev = eval(models[3][model_choice])\n",
    "print()\n",
    "print(model, features, ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "output_df = get_bets(model, features, ev, df_train, label_train, odds_train, df_test, label_test,\n",
    "                         odds_test, verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df = output_df[['t1_name', 't2_name', 't1_prob', 't2_prob']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = return_df.rename(columns = {'t1_name': 'R_fighter', 't2_name': 'B_fighter', 't1_prob': 'R_prob', 't2_prob': 'B_prob'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_csv('upcoming_probs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
