{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d987350",
   "metadata": {},
   "source": [
    "# The goal here is to prep a dataframe with all the probabilities for the different methods of victory with the result and extra info.  This will be used for:\n",
    " 1. Visualization\n",
    " 2. Stacking Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ec8659",
   "metadata": {},
   "source": [
    "This is based on the update model scores methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b6ef675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from datetime import date\n",
    "#from sklearn.mixture import DPGMM\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953a036d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46ef99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../automated_model_creation') #We need to access the function file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b723e1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c71eb239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THESE ARE THE 2 VARIABLES WE SET\n",
    "is_prod = True\n",
    "model_num = 0\n",
    "\n",
    "\n",
    "\n",
    "if is_prod:\n",
    "    model_file = '../../../data/production_models_mov.csv'\n",
    "else: #If not production then test\n",
    "    model_file = '../../../data/models_mov.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27862ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "with open(model_file, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    models = list(reader)\n",
    "    \n",
    "print(len(models[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7def8328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../../data/production_models_mov.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../../data/kaggle_data/ufc-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b1cfc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cec7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_finish_type(winner, finish):\n",
    "    #print(winner, finish)\n",
    "    #Why overcomplicate things?  We can just use a few if statements\n",
    "    if winner == 'Red':\n",
    "        #print(\"HI\")\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Red - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Red - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Red - KO/TKO')\n",
    "    if winner == 'Blue':\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Blue - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Blue - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Blue - KO/TKO')\n",
    "        \n",
    "    #Test for NaN\n",
    "    if finish != finish:\n",
    "        return('')\n",
    "    \n",
    "    if finish == 'Overturned':\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "    return ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2b9acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calls for the power of lambda!\n",
    "df['finish_type'] = df.apply(lambda x: return_finish_type(x['Winner'], x['finish']), axis=1)\n",
    "mask = df['finish_type'] != ''\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77f499a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-af8f7542de83>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = f\n"
     ]
    }
   ],
   "source": [
    "finish_list = ['Red - DEC', 'Red - SUB', 'Red - KO/TKO', 'Blue - DEC', 'Blue - SUB', 'Blue - KO/TKO']\n",
    "\n",
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "\n",
    "for f in range(len(finish_list)):\n",
    "    mask = df['finish_type'] == finish_list[f]\n",
    "    df['label'][mask] = f\n",
    "    \n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55e71de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'r_dec_odds': 'Red - DEC', 'r_sub_odds': 'Red - SUB', 'r_ko_odds': 'Red - KO/TKO',\n",
    "                'b_dec_odds': 'Blue - DEC', 'b_sub_odds': 'Blue - SUB', 'b_ko_odds': 'Blue - KO/TKO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e18764ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df['label']\n",
    "odds_df = df[finish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24010db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[250:]\n",
    "odds_train = odds_df[250:]\n",
    "label_train = label_df[250:]\n",
    "\n",
    "df_test = df[:250]\n",
    "odds_test = odds_df[:250]\n",
    "label_test = label_df[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2918c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to clean\n",
    "mask = df_train['finish_type'] != ''\n",
    "df_train = df_train[mask]\n",
    "#print(len(df_train))\n",
    "\n",
    "mask = df_test['finish_type'] != ''\n",
    "df_test = df_test[mask]\n",
    "#print(len(df_test))\n",
    "\n",
    "label_train = label_train[label_train.index.isin(df_train.index)]\n",
    "label_test = label_test[label_test.index.isin(df_test.index)]\n",
    "\n",
    "odds_train = odds_train[odds_train.index.isin(df_train.index)]\n",
    "odds_test = odds_test[odds_test.index.isin(df_test.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1f92182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a value for the nulls in the ranks\n",
    "\n",
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da296d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f531e197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK WE NEED TO CREATE A NEW VERSION OF evaluate_model_mov that also returns a dataframe of probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f50fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bets_dataframe(df_odds, probs, labels, label_list, probs_label_list, print_stats = False, min_ev = 0, get_total=True):\n",
    "    probs_label_list = [int(a) for a in probs_label_list]\n",
    "    #labels = [int(a) for a in labels]\n",
    "    \n",
    "    bets_list = []\n",
    "    df_odds.reset_index(drop=True, inplace=True)\n",
    "    labels.reset_index(drop=True, inplace=True)\n",
    "    score = 0\n",
    "    #print(df_odds)\n",
    "    for i in range(len(df_odds)):\n",
    "        #print(i)\n",
    "        #        df_temp_odds = df_odds.iloc[[i, :]]\n",
    "        #print(df_odds.iloc[[i]])\n",
    "        for l in range(len(probs[i])):\n",
    "            #print(f\"{label_list[probs_label_list[l]]}: {probs[i][l]}\")\n",
    "            temp_odds = (df_odds.loc[[i]])[label_list[probs_label_list[l]]][i]\n",
    "            #print((temp_odds))\n",
    "            bet_ev = get_bet_ev(temp_odds, probs[i][l])\n",
    "            #print(bet_ev)\n",
    "            if bet_ev > min_ev:\n",
    "                #print(l)\n",
    "                if labels[i] == probs_label_list[l]:\n",
    "                    #print(f\"{int(labels[i])} {probs_label_list[l]}\")\n",
    "                    score = score + get_bet_return(temp_odds)\n",
    "                    temp_score = get_bet_return(temp_odds)\n",
    "                    #print(f\"Winning Bet. New Score: {score}\")\n",
    "                else:\n",
    "                    score = score - 100\n",
    "                    temp_score = -100\n",
    "                    #print(f\"Losing Bet.  New Score: {score}\")\n",
    "                #print(f\"{labels[i]} {probs_label_list[l]} {probs[i][l]} {temp_odds} {temp_score} {label_list[labels[i]]}\")\n",
    "                bets_list.append([labels[i], probs_label_list[l], probs[i][l], temp_odds, temp_score, label_list[labels[i]], label_list[probs_label_list[l]] ],)\n",
    "            #print()\n",
    "            \n",
    "            \n",
    "            \n",
    "        #print(f\"Result: {label_list[int(labels[i])]} ({int(labels[i])})\")\n",
    "    print(\"Real Score: \" + str(score))\n",
    "    return(score, bets_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ac23b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dataframe(input_model, input_features, input_ev, train_df, train_labels, train_odds, test_df, test_labels,\n",
    "                  test_odds, label_list, verbose=True):\n",
    "    model_score = 0\n",
    "    \n",
    "    df_train = train_df[input_features].copy()\n",
    "    df_test = test_df[input_features].copy()\n",
    "    df_train = df_train.dropna()\n",
    "    df_test = df_test.dropna()\n",
    "    \n",
    "    df_train = pd.get_dummies(df_train)\n",
    "    df_test = pd.get_dummies(df_test)\n",
    "    df_train, df_test = df_train.align(df_test, join='left', axis=1)    #Ensures both sets are dummified the same\n",
    "    df_test = df_test.fillna(0)\n",
    "\n",
    "    labels_train = train_labels[train_labels.index.isin(df_train.index)]\n",
    "    odds_train = train_odds[train_odds.index.isin(df_train.index)] \n",
    "    labels_test = test_labels[test_labels.index.isin(df_test.index)]\n",
    "    odds_test = test_odds[test_odds.index.isin(df_test.index)] \n",
    "    \n",
    "    odds_train = odds_train.dropna()\n",
    "    odds_test = odds_test.dropna()\n",
    "    \n",
    "    df_train = df_train[df_train.index.isin(odds_train.index)]\n",
    "    df_test = df_test[df_test.index.isin(odds_test.index)]\n",
    "    \n",
    "    labels_train = labels_train[labels_train.index.isin(odds_train.index)]\n",
    "    labels_test = labels_test[labels_test.index.isin(odds_test.index)]    \n",
    "    \n",
    "    \n",
    "\n",
    "    if verbose:\n",
    "        display(df_train.shape)\n",
    "        display(labels_train.shape)\n",
    "        display(odds_train.shape)\n",
    "        display(df_test.shape)\n",
    "        display(labels_test.shape)\n",
    "        display(odds_test.shape)\n",
    "\n",
    "    #print(labels_train)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaled_train = scaler.fit_transform(df_train)\n",
    "    \n",
    "    input_model.fit(scaled_train, labels_train)\n",
    "\n",
    "    scaled_test = scaler.transform(df_test)\n",
    "    \n",
    "    probs = input_model.predict_proba(scaled_test)\n",
    "    model_score, bets_list = get_bets_dataframe(odds_test, probs, labels_test, label_list, input_model.classes_, print_stats = True, min_ev = input_ev, get_total=True)\n",
    "\n",
    "    #print((odds_test))\n",
    "    #print(probs)\n",
    "    #print(label_list)\n",
    "    #print(input_model.classes_)\n",
    "    #print(label_test)\n",
    "    #print(bets_list)\n",
    "    #Let's turn bets list into a dataframe\n",
    "    #print(f\"{labels[i]} {probs_label_list[l]} {probs[i][l]} {temp_odds} {temp_score} {label_list[labels[i]]}\")\n",
    "\n",
    "    df = pd.DataFrame.from_records(bets_list, columns=['label_code', 'bet_code', 'probability', 'odds', 'score', 'result', 'bet'])\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb07251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2834, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2834,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2834, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real Score: 8205.0\n"
     ]
    }
   ],
   "source": [
    "test_model_name = models[0][model_num]\n",
    "test_model = eval(models[1][model_num])\n",
    "test_model_features = eval(models[2][model_num])\n",
    "test_model_ev = eval(models[3][model_num])\n",
    "model_result_df = (get_model_dataframe(test_model, test_model_features, test_model_ev, df_train, label_train, odds_train, df_test, label_test,\n",
    "                     odds_test, finish_list, verbose = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fa08591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc4f8717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c069d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-28\n"
     ]
    }
   ],
   "source": [
    "d = date.today()\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "174f924c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_results_2021-08-28.csv\n"
     ]
    }
   ],
   "source": [
    "f = \"model_results_\" + str(d) + \".csv\"\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ff60bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59ee7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
