{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b3eeba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408a330b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kaggle_data/ufc-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a34717c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90f34471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_finish_type(winner, finish):\n",
    "    #print(winner, finish)\n",
    "    #Why overcomplicate things?  We can just use a few if statements\n",
    "    if winner == 'Red':\n",
    "        #print(\"HI\")\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Red - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Red - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Red - KO/TKO')\n",
    "    if winner == 'Blue':\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Blue - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Blue - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Blue - KO/TKO')\n",
    "        \n",
    "    #Test for NaN\n",
    "    if finish != finish:\n",
    "        return('')\n",
    "    \n",
    "    if finish == 'Overturned':\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "    return ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "979a334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calls for the power of lambda!\n",
    "df['finish_type'] = df.apply(lambda x: return_finish_type(x['Winner'], x['finish']), axis=1)\n",
    "mask = df['finish_type'] != ''\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab1a77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-af8f7542de83>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = f\n"
     ]
    }
   ],
   "source": [
    "finish_list = ['Red - DEC', 'Red - SUB', 'Red - KO/TKO', 'Blue - DEC', 'Blue - SUB', 'Blue - KO/TKO']\n",
    "\n",
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "\n",
    "for f in range(len(finish_list)):\n",
    "    mask = df['finish_type'] == finish_list[f]\n",
    "    df['label'][mask] = f\n",
    "    \n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45cfafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'r_dec_odds': 'Red - DEC', 'r_sub_odds': 'Red - SUB', 'r_ko_odds': 'Red - KO/TKO',\n",
    "                'b_dec_odds': 'Blue - DEC', 'b_sub_odds': 'Blue - SUB', 'b_ko_odds': 'Blue - KO/TKO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed22aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df['label']\n",
    "odds_df = df[finish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3555e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "250\n",
      "4293\n",
      "4293\n",
      "4293\n"
     ]
    }
   ],
   "source": [
    "#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n",
    "#as we pick models\n",
    "\n",
    "df_train = df[250:]\n",
    "odds_train = odds_df[250:]\n",
    "label_train = label_df[250:]\n",
    "\n",
    "df_test = df[:250]\n",
    "odds_test = odds_df[:250]\n",
    "label_test = label_df[:250]\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(odds_test))\n",
    "print(len(label_test))\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(odds_train))\n",
    "print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cd3962e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293\n",
      "4293\n",
      "4293\n",
      "250\n",
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "#We need to clean\n",
    "mask = df_train['finish_type'] != ''\n",
    "df_train = df_train[mask]\n",
    "#print(len(df_train))\n",
    "\n",
    "mask = df_test['finish_type'] != ''\n",
    "df_test = df_test[mask]\n",
    "#print(len(df_test))\n",
    "\n",
    "label_train = label_train[label_train.index.isin(df_train.index)]\n",
    "label_test = label_test[label_test.index.isin(df_test.index)]\n",
    "\n",
    "odds_train = odds_train[odds_train.index.isin(df_train.index)]\n",
    "odds_test = odds_test[odds_test.index.isin(df_test.index)]\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(label_train))\n",
    "print(len(odds_train))\n",
    "print(len(df_test))\n",
    "print(len(label_test))\n",
    "print(len(odds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c90cd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a178e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name = \"dummy_model_for_trace\"\n",
    "test_model = DecisionTreeClassifier(random_state=75, min_samples_leaf=0.01)\n",
    "test_model_features = ['B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT',\n",
    "       'B_avg_TD_landed', 'B_avg_TD_pct', 'B_longest_win_streak', 'B_losses',\n",
    "       'B_total_rounds_fought', 'B_total_title_bouts',\n",
    "       'B_win_by_Decision_Majority', 'B_win_by_Decision_Split',\n",
    "       'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO']\n",
    "test_model_ev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5cc4306",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test_model = test_model\n",
    "old_test_model_features = test_model_features\n",
    "old_test_model_ev = test_model_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f21974e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_features = ['R_odds', 'B_odds', 'R_ev', 'B_ev',\n",
    "       'location', 'country', 'title_bout', 'weight_class', 'gender',\n",
    "       'no_of_rounds', 'B_current_lose_streak', 'B_current_win_streak',\n",
    "       'B_draw', 'B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT',\n",
    "       'B_avg_TD_landed', 'B_avg_TD_pct', 'B_longest_win_streak', 'B_losses',\n",
    "       'B_total_rounds_fought', 'B_total_title_bouts',\n",
    "       'B_win_by_Decision_Majority', 'B_win_by_Decision_Split',\n",
    "       'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO', 'B_win_by_Submission',\n",
    "       'B_win_by_TKO_Doctor_Stoppage', 'B_wins', 'B_Stance', 'B_Height_cms',\n",
    "       'B_Reach_cms', 'B_Weight_lbs', 'R_current_lose_streak',\n",
    "       'R_current_win_streak', 'R_draw', 'R_avg_SIG_STR_landed',\n",
    "       'R_avg_SIG_STR_pct', 'R_avg_SUB_ATT', 'R_avg_TD_landed', 'R_avg_TD_pct',\n",
    "       'R_longest_win_streak', 'R_losses', 'R_total_rounds_fought',\n",
    "       'R_total_title_bouts', 'R_win_by_Decision_Majority',\n",
    "       'R_win_by_Decision_Split', 'R_win_by_Decision_Unanimous',\n",
    "       'R_win_by_KO/TKO', 'R_win_by_Submission',\n",
    "       'R_win_by_TKO_Doctor_Stoppage', 'R_wins', 'R_Stance', 'R_Height_cms',\n",
    "       'R_Reach_cms', 'R_Weight_lbs', 'R_age', 'B_age', 'lose_streak_dif',\n",
    "       'win_streak_dif', 'longest_win_streak_dif', 'win_dif', 'loss_dif',\n",
    "       'total_round_dif', 'total_title_bout_dif', 'ko_dif', 'sub_dif',\n",
    "       'height_dif', 'reach_dif', 'age_dif', 'sig_str_dif', 'avg_sub_att_dif',\n",
    "       'avg_td_dif', 'empty_arena', 'B_match_weightclass_rank', 'R_match_weightclass_rank', \n",
    "        \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\",\n",
    "        \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', \n",
    "        'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', \n",
    "        'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \n",
    "        \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", \n",
    "        'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', \n",
    "        'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', \n",
    "        'B_Pound-for-Pound_rank', 'Red - DEC', 'Blue - DEC', 'Red - SUB', 'Blue - SUB', 'Red - KO/TKO', 'Blue - KO/TKO', 'better_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9acc9d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_model_for_trace\n",
      "DecisionTreeClassifier(min_samples_leaf=0.01, random_state=75)\n",
      "['B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT', 'B_avg_TD_landed', 'B_avg_TD_pct', 'B_longest_win_streak', 'B_losses', 'B_total_rounds_fought', 'B_total_title_bouts', 'B_win_by_Decision_Majority', 'B_win_by_Decision_Split', 'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_model_name)\n",
    "print(test_model)\n",
    "print(test_model_features)\n",
    "print(test_model_ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acfa8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model():\n",
    "    print()\n",
    "    print(test_model_name)\n",
    "    print(test_model)\n",
    "    print(test_model_features)\n",
    "    print(test_model_ev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3bde6a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ev(input_df, input_model, input_features, input_labels, odds_input, min_ev = 0, verbose=False, get_total=True):\n",
    "    df_sel = input_df[input_features]\n",
    "    df_sel = df_sel.dropna()\n",
    "    df_sel = pd.get_dummies(df_sel)\n",
    "    labels_sel = input_labels[input_labels.index.isin(df_sel.index)]\n",
    "    odds_sel = odds_input[odds_input.index.isin(df_sel.index)] \n",
    "    best_score = custom_cv_eval(df_sel, input_model, labels_sel, odds_sel, min_ev = min_ev, verbose=verbose, \n",
    "                                get_total=get_total)\n",
    "    return best_score\n",
    "\n",
    "\n",
    "#Input: American Odds, and Probability of a Winning Bet\n",
    "#Output: Bet EV based on a $100 bet\n",
    "def get_bet_ev(odds, prob):\n",
    "    if odds>0:\n",
    "        return ((odds * prob) - (100 * (1-prob)) )\n",
    "    else:\n",
    "        return ((100 / abs(odds))*100*prob - (100 * (1-prob)))\n",
    "    \n",
    "def get_bet_return(odds):\n",
    "    if odds>0:\n",
    "        return odds\n",
    "    else:\n",
    "        return (100 / abs(odds))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66dc041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ev_for_optimize_mov(df_odds, probs, labels,  print_stats = False, min_ev = 0, get_total=True):\n",
    "        \n",
    "    score = 0\n",
    "    #print(df_odds)\n",
    "    for i in range(len(df_odds)):\n",
    "        #print(i)\n",
    "        #        df_temp_odds = df_odds.iloc[[i, :]]\n",
    "        #print()\n",
    "        #print()\n",
    "        #print(df_odds[i])\n",
    "        for l in range(len(probs[i])):\n",
    "            temp_odds = (df_odds[i][l])\n",
    "            #print((temp_odds))\n",
    "            bet_ev = get_bet_ev(temp_odds, probs[i][l])\n",
    "            #print(bet_ev)\n",
    "            if bet_ev > min_ev:\n",
    "                #print(l)\n",
    "                if labels[i] == l:\n",
    "                    #print(f\"{int(labels[i])} {l}\")\n",
    "                    score = score + get_bet_return(temp_odds)\n",
    "                    #print(f\"Winning Bet. New Score: {score}\")\n",
    "                else:\n",
    "                    score = score - 100\n",
    "                    #print(f\"Losing Bet.  New Score: {score}\")\n",
    "                    \n",
    "            #print()\n",
    "            \n",
    "            \n",
    "            \n",
    "        #print(f\"Result: {labels[i]}\")\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b3c1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_cv_eval_mov(df, m, labels, odds, min_ev=0, verbose=False, get_total=True):\n",
    "    #If we have less than 5 samples we are going to break the split.\n",
    "    #print(\"HI\")\n",
    "    if len(df) < 5:\n",
    "        return 0\n",
    "    X = np.array(df)\n",
    "    y = np.array(labels)\n",
    "    odds = np.array(odds)\n",
    "    running_total = 0\n",
    "    count=1\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=75)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        odds_train, odds_test = odds[train_index], odds[test_index]\n",
    "        #display(y_train)\n",
    "        scaler = StandardScaler()\n",
    "        scaled_train = scaler.fit_transform(X_train)\n",
    "        scaled_test = scaler.transform(X_test)\n",
    "        \n",
    "        m.fit(scaled_train, y_train)\n",
    "        probs=m.predict_proba(scaled_test)\n",
    "        #print(probs)\n",
    "        #We need to prep the dataframe to evaluate....\n",
    "        #X_odds = X_test[['t1_odds', 't2_odds']]\n",
    "        #print(X_test)\n",
    "        #print(X_test[:, -1])\n",
    "        #print(X_test[:, -2])\n",
    "        #X_odds = list(zip(odds_test[:, -2], odds_test[:, -1], probs[:, 0], probs[:, 1], y_test))\n",
    "        #ev_prepped_df = pd.DataFrame(X_odds, columns=['t1_odds', 't2_odds', 't1_prob', 't2_prob', 'winner'])\n",
    "        #display(ev_prepped_df)\n",
    "        #display(temp_df)\n",
    "        #print(f\"{count}: {get_ev_from_df(ev_prepped_df, print_stats = False)}\")\n",
    "        count=count+1\n",
    "        running_total = running_total + get_ev_for_optimize_mov(odds_test, probs, y_test,  min_ev= min_ev, get_total=get_total )\n",
    "\n",
    "        #display(ev_prepped_df)\n",
    "    \n",
    "    return running_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ev(input_df, input_model, input_features, input_labels, odds_input, min_ev = 0, verbose=False, get_total=True):\n",
    "    df_sel = input_df[input_features]\n",
    "    df_sel = df_sel.dropna()\n",
    "    df_sel = pd.get_dummies(df_sel)\n",
    "    labels_sel = input_labels[input_labels.index.isin(df_sel.index)]\n",
    "    odds_sel = odds_input[odds_input.index.isin(df_sel.index)] \n",
    "    print(len(odds_sel.columns))\n",
    "    if len(odds_sel.columns) == 6:\n",
    "        best_score = custom_cv_eval_mov(df_sel, input_model, labels_sel, odds_sel, min_ev = min_ev, verbose=verbose, \n",
    "                                get_total=get_total)        \n",
    "    else:\n",
    "        best_score = custom_cv_eval(df_sel, input_model, labels_sel, odds_sel, min_ev = min_ev, verbose=verbose, \n",
    "                                get_total=get_total)\n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a212eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83ff857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_to_improve_mov(cur_features, m, df, labels, odds, scale=False, min_ev = 0):\n",
    "    number_of_features = len(cur_features)\n",
    "    df_sel = df[cur_features]\n",
    "    df_sel = df_sel.dropna()\n",
    "    df_sel = pd.get_dummies(df_sel)\n",
    "    labels_sel = labels[labels.index.isin(df_sel.index)]\n",
    "    odds_sel = odds[odds.index.isin(df_sel.index)]        \n",
    "    orig_score = custom_cv_eval_mov(df_sel, m, labels_sel, odds_sel, get_total=True, min_ev = min_ev)    \n",
    "    best_features = cur_features\n",
    "    best_score = orig_score\n",
    "    print(f\"The original score is {orig_score}\")\n",
    "    if number_of_features == 0:\n",
    "        return []\n",
    "    if number_of_features == 1:\n",
    "        return cur_features\n",
    "    \n",
    "    for z in range(number_of_features):\n",
    "        temp_feature = df.columns[z]\n",
    "        temp_features = cur_features.copy()\n",
    "        #Remove a feature\n",
    "        del temp_features[z]\n",
    "        df_sel = df[temp_features]\n",
    "        df_sel = df_sel.dropna()\n",
    "        df_sel = pd.get_dummies(df_sel)\n",
    "        labels_sel = labels[labels.index.isin(df_sel.index)]\n",
    "        odds_sel = odds[odds.index.isin(df_sel.index)]        \n",
    "        temp_score = custom_cv_eval_mov(df_sel, m, labels_sel, odds_sel, get_total=True, min_ev = min_ev)\n",
    "        if temp_score > best_score:\n",
    "            best_features = temp_features\n",
    "            best_score = temp_score\n",
    "            print(f\"NEW BEST FEATURE SET WITH: \" + temp_feature + \" REMOVED \" + str(best_score))\n",
    "        \n",
    "        #Get a score\n",
    "    if best_features != cur_features:\n",
    "        return remove_to_improve_mov(best_features, m, df, labels, odds, scale, min_ev)\n",
    "    else:\n",
    "        return best_features    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "75f61a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original score is -142165.25746802217\n",
      "NEW BEST FEATURE SET WITH: B_ev REMOVED -134623.38195438386\n",
      "The original score is -134623.38195438386\n",
      "NEW BEST FEATURE SET WITH: title_bout REMOVED -133470.30094628307\n",
      "The original score is -133470.30094628307\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['B_avg_SIG_STR_landed',\n",
       " 'B_avg_SIG_STR_pct',\n",
       " 'B_avg_SUB_ATT',\n",
       " 'B_avg_TD_landed',\n",
       " 'B_avg_TD_pct',\n",
       " 'B_losses',\n",
       " 'B_total_rounds_fought',\n",
       " 'B_total_title_bouts',\n",
       " 'B_win_by_Decision_Majority',\n",
       " 'B_win_by_Decision_Split',\n",
       " 'B_win_by_KO/TKO']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_to_improve_mov(test_model_features, test_model, df_train, label_train, odds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56379c73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
