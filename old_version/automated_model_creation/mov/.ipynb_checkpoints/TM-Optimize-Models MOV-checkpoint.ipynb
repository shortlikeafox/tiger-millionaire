{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../automated_model_creation') #We need to access the function file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import *\n",
    "import random\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#from sklearn.mixture import DPGMM\n",
    "\n",
    "remove_fight_island = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn off warnings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load models\n",
    "#REMINDER: We are going to need to use 'eval' to get the models usable\n",
    "with open('../../data/models_mov.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    models = list(reader)\n",
    "    \n",
    "#print(len(models))\n",
    "\n",
    "\n",
    "###SELECT MODEL TO OPTIMIZE\n",
    "#model_num = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/kaggle_data/ufc-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_finish_type(winner, finish):\n",
    "    #print(winner, finish)\n",
    "    #Why overcomplicate things?  We can just use a few if statements\n",
    "    if winner == 'Red':\n",
    "        #print(\"HI\")\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Red - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Red - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Red - KO/TKO')\n",
    "    if winner == 'Blue':\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Blue - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Blue - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Blue - KO/TKO')\n",
    "        \n",
    "    #Test for NaN\n",
    "    if finish != finish:\n",
    "        return('')\n",
    "    \n",
    "    if finish == 'Overturned':\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "    return ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calls for the power of lambda!\n",
    "df['finish_type'] = df.apply(lambda x: return_finish_type(x['Winner'], x['finish']), axis=1)\n",
    "mask = df['finish_type'] != ''\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "finish_list = ['Red - DEC', 'Red - SUB', 'Red - KO/TKO', 'Blue - DEC', 'Blue - SUB', 'Blue - KO/TKO']\n",
    "\n",
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "\n",
    "for f in range(len(finish_list)):\n",
    "    mask = df['finish_type'] == finish_list[f]\n",
    "    df['label'][mask] = f\n",
    "    \n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'r_dec_odds': 'Red - DEC', 'r_sub_odds': 'Red - SUB', 'r_ko_odds': 'Red - KO/TKO',\n",
    "                'b_dec_odds': 'Blue - DEC', 'b_sub_odds': 'Blue - SUB', 'b_ko_odds': 'Blue - KO/TKO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df['label']\n",
    "odds_df = df[finish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n",
    "#as we pick models\n",
    "\n",
    "df_train = df[250:]\n",
    "odds_train = odds_df[250:]\n",
    "label_train = label_df[250:]\n",
    "\n",
    "df_test = df[:250]\n",
    "odds_test = odds_df[:250]\n",
    "label_test = label_df[:250]\n",
    "\n",
    "#print(len(df_test))\n",
    "#print(len(odds_test))\n",
    "#print(len(label_test))\n",
    "\n",
    "#print(len(df_train))\n",
    "#print(len(odds_train))\n",
    "#print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to clean\n",
    "mask = df_train['finish_type'] != ''\n",
    "df_train = df_train[mask]\n",
    "#print(len(df_train))\n",
    "\n",
    "mask = df_test['finish_type'] != ''\n",
    "df_test = df_test[mask]\n",
    "#print(len(df_test))\n",
    "\n",
    "label_train = label_train[label_train.index.isin(df_train.index)]\n",
    "label_test = label_test[label_test.index.isin(df_test.index)]\n",
    "\n",
    "odds_train = odds_train[odds_train.index.isin(df_train.index)]\n",
    "odds_test = odds_test[odds_test.index.isin(df_test.index)]\n",
    "\n",
    "\n",
    "#print(len(df_train))\n",
    "#print(len(label_train))\n",
    "#print(len(odds_train))\n",
    "#print(len(df_test))\n",
    "#print(len(label_test))\n",
    "#print(len(odds_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_fight_island:\n",
    "    ##Let's remove the Fight island contests and see how that affects score\n",
    "    df_test_no_fight_island = df_test[(df_test['location'] != 'Abu Dhabi, Abu Dhabi, United Arab Emirates')]\n",
    "    df_train_no_fight_island = df_train[(df_train['location'] != 'Abu Dhabi, Abu Dhabi, United Arab Emirates')]\n",
    "    df_test = df_test_no_fight_island\n",
    "    df_train = df_train_no_fight_island\n",
    "\n",
    "\n",
    "\n",
    "#print(len(df_test))\n",
    "#print(len(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(df_train)\n",
    "#display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set a value for the nulls in the ranks\n",
    "\n",
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df_test\n",
    "#df_test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Set features\n",
    "#2. Set Hyperparameters\n",
    "#3. Set EV\n",
    "#4. Remove Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LinearDiscriminantAnalysis(solver='lsqr')\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models[1][model_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name = models[0][model_num]\n",
    "test_model = eval(models[1][model_num])\n",
    "test_model_features = eval(models[2][model_num])\n",
    "test_model_ev = eval(models[3][model_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test_model = test_model\n",
    "old_test_model_features = test_model_features\n",
    "old_test_model_ev = test_model_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. set features\n",
    "my_pos_features = ['R_odds', 'B_odds', 'R_ev', 'B_ev',\n",
    "       'location', 'country', 'title_bout', 'weight_class', 'gender',\n",
    "       'no_of_rounds', 'B_current_lose_streak', 'B_current_win_streak',\n",
    "       'B_draw', 'B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT',\n",
    "       'B_avg_TD_landed', 'B_avg_TD_pct', 'B_longest_win_streak', 'B_losses',\n",
    "       'B_total_rounds_fought', 'B_total_title_bouts',\n",
    "       'B_win_by_Decision_Majority', 'B_win_by_Decision_Split',\n",
    "       'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO', 'B_win_by_Submission',\n",
    "       'B_win_by_TKO_Doctor_Stoppage', 'B_wins', 'B_Stance', 'B_Height_cms',\n",
    "       'B_Reach_cms', 'B_Weight_lbs', 'R_current_lose_streak',\n",
    "       'R_current_win_streak', 'R_draw', 'R_avg_SIG_STR_landed',\n",
    "       'R_avg_SIG_STR_pct', 'R_avg_SUB_ATT', 'R_avg_TD_landed', 'R_avg_TD_pct',\n",
    "       'R_longest_win_streak', 'R_losses', 'R_total_rounds_fought',\n",
    "       'R_total_title_bouts', 'R_win_by_Decision_Majority',\n",
    "       'R_win_by_Decision_Split', 'R_win_by_Decision_Unanimous',\n",
    "       'R_win_by_KO/TKO', 'R_win_by_Submission',\n",
    "       'R_win_by_TKO_Doctor_Stoppage', 'R_wins', 'R_Stance', 'R_Height_cms',\n",
    "       'R_Reach_cms', 'R_Weight_lbs', 'R_age', 'B_age', 'lose_streak_dif',\n",
    "       'win_streak_dif', 'longest_win_streak_dif', 'win_dif', 'loss_dif',\n",
    "       'total_round_dif', 'total_title_bout_dif', 'ko_dif', 'sub_dif',\n",
    "       'height_dif', 'reach_dif', 'age_dif', 'sig_str_dif', 'avg_sub_att_dif',\n",
    "       'avg_td_dif', 'empty_arena', 'B_match_weightclass_rank', 'R_match_weightclass_rank', \n",
    "        \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\",\n",
    "        \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', \n",
    "        'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', \n",
    "        'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \n",
    "        \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", \n",
    "        'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', \n",
    "        'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', \n",
    "        'B_Pound-for-Pound_rank', 'Red - DEC', 'Blue - DEC', 'Red - SUB', 'Blue - SUB', 'Red - KO/TKO', 'Blue - KO/TKO', 'better_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_15\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "[\"R_Women's Featherweight_rank\", 'B_Lightweight_rank', 'R_Featherweight_rank', 'R_Light Heavyweight_rank', 'total_title_bout_dif', 'R_losses', 'B_win_by_Submission', 'Blue - DEC', 'B_draw', 'R_Middleweight_rank', 'B_avg_SIG_STR_pct', \"B_Women's Strawweight_rank\", 'age_dif', 'B_Middleweight_rank', 'avg_sub_att_dif', 'avg_td_dif', 'B_avg_SUB_ATT', \"R_Women's Flyweight_rank\", 'R_Pound-for-Pound_rank', 'R_Reach_cms', 'R_wins', 'B_ev', 'R_Weight_lbs', 'sub_dif', 'R_ev', 'B_avg_TD_landed']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_model_name)\n",
    "print(test_model)\n",
    "print(test_model_features)\n",
    "print(test_model_ev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    score = evaluate_model_mov(test_model, test_model_features, test_model_ev, df_train, label_train, odds_train, df_test, label_test,\n",
    "                         odds_test, finish_list, verbose = True)\n",
    "    models[0][model_num] =  test_model_name \n",
    "    models[1][model_num] = test_model\n",
    "    models[2][model_num] = test_model_features\n",
    "    models[3][model_num] = test_model_ev\n",
    "    models[4][model_num] = score    \n",
    "    \n",
    "    with open('../../data/models_mov.csv', 'w', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        for row in models:\n",
    "            print(\"HI\")\n",
    "            writer.writerow(row)\n",
    "\n",
    "    outfile.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model():\n",
    "    print()\n",
    "    print(test_model_name)\n",
    "    print(test_model)\n",
    "    print(test_model_features)\n",
    "    print(test_model_ev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test evaluate model\n",
    "#odds_test\n",
    "\n",
    "#print(evaluate_model_mov(test_model, test_model_features, test_model_ev, df_train, label_train, odds_train, df_test, label_test, odds_test, finish_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current best score is: -5713.7552272536395\n",
      "NO IMPROVEMENT\n",
      "FINAL BEST SCORE: -5713.7552272536395\n",
      "\n",
      "model_15\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "[\"R_Women's Featherweight_rank\", 'B_Lightweight_rank', 'R_Featherweight_rank', 'R_Light Heavyweight_rank', 'total_title_bout_dif', 'R_losses', 'B_win_by_Submission', 'Blue - DEC', 'B_draw', 'R_Middleweight_rank', 'B_avg_SIG_STR_pct', \"B_Women's Strawweight_rank\", 'age_dif', 'B_Middleweight_rank', 'avg_sub_att_dif', 'avg_td_dif', 'B_avg_SUB_ATT', \"R_Women's Flyweight_rank\", 'R_Pound-for-Pound_rank', 'R_Reach_cms', 'R_wins', 'B_ev', 'R_Weight_lbs', 'sub_dif', 'R_ev', 'B_avg_TD_landed']\n",
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2782, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250     0\n",
      "252     5\n",
      "253     2\n",
      "254     3\n",
      "255     0\n",
      "       ..\n",
      "4162    0\n",
      "4170    1\n",
      "4171    3\n",
      "4172    5\n",
      "4173    5\n",
      "Name: label, Length: 2782, dtype: int64\n",
      "Real Score: 6180.0\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "\n",
      "\n",
      "Starting New Run for LinearDiscriminantAnalysis\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "\n",
      "\n",
      "Previous Best Score: -5713.7552272536395\n",
      "solver:  svd tol:  0.0001 Score:  -6247.72074449502\n",
      "solver:  svd tol:  0.00011000000000000002 Score:  -6247.72074449502\n",
      "solver:  svd tol:  9e-05 Score:  -6247.72074449502\n",
      "solver:  lsqr tol:  0.0001 Score:  -5713.7552272536395\n",
      "solver:  lsqr tol:  0.00011000000000000002 Score:  -5713.7552272536395\n",
      "solver:  lsqr tol:  9e-05 Score:  -5713.7552272536395\n",
      "output model: LinearDiscriminantAnalysis(solver='lsqr')\n",
      "pos model: LinearDiscriminantAnalysis(solver='lsqr')\n",
      " output_model: LinearDiscriminantAnalysis(solver='lsqr')\n",
      "Real output model: LinearDiscriminantAnalysis(solver='lsqr')\n",
      "\n",
      "model_15\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "[\"R_Women's Featherweight_rank\", 'B_Lightweight_rank', 'R_Featherweight_rank', 'R_Light Heavyweight_rank', 'total_title_bout_dif', 'R_losses', 'B_win_by_Submission', 'Blue - DEC', 'B_draw', 'R_Middleweight_rank', 'B_avg_SIG_STR_pct', \"B_Women's Strawweight_rank\", 'age_dif', 'B_Middleweight_rank', 'avg_sub_att_dif', 'avg_td_dif', 'B_avg_SUB_ATT', \"R_Women's Flyweight_rank\", 'R_Pound-for-Pound_rank', 'R_Reach_cms', 'R_wins', 'B_ev', 'R_Weight_lbs', 'sub_dif', 'R_ev', 'B_avg_TD_landed']\n",
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2782, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250     0\n",
      "252     5\n",
      "253     2\n",
      "254     3\n",
      "255     0\n",
      "       ..\n",
      "4162    0\n",
      "4170    1\n",
      "4171    3\n",
      "4172    5\n",
      "4173    5\n",
      "Name: label, Length: 2782, dtype: int64\n",
      "Real Score: 6180.0\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "\n",
      "model_15\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "[\"R_Women's Featherweight_rank\", 'B_Lightweight_rank', 'R_Featherweight_rank', 'R_Light Heavyweight_rank', 'total_title_bout_dif', 'R_losses', 'B_win_by_Submission', 'Blue - DEC', 'B_draw', 'R_Middleweight_rank', 'B_avg_SIG_STR_pct', \"B_Women's Strawweight_rank\", 'age_dif', 'B_Middleweight_rank', 'avg_sub_att_dif', 'avg_td_dif', 'B_avg_SUB_ATT', \"R_Women's Flyweight_rank\", 'R_Pound-for-Pound_rank', 'R_Reach_cms', 'R_wins', 'B_ev', 'R_Weight_lbs', 'sub_dif', 'R_ev', 'B_avg_TD_landed']\n",
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2782, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250     0\n",
      "252     5\n",
      "253     2\n",
      "254     3\n",
      "255     0\n",
      "       ..\n",
      "4162    0\n",
      "4170    1\n",
      "4171    3\n",
      "4172    5\n",
      "4173    5\n",
      "Name: label, Length: 2782, dtype: int64\n",
      "Real Score: 6180.0\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "\n",
      "model_15\n",
      "LinearDiscriminantAnalysis(solver='lsqr')\n",
      "[\"R_Women's Featherweight_rank\", 'B_Lightweight_rank', 'R_Featherweight_rank', 'R_Light Heavyweight_rank', 'total_title_bout_dif', 'R_losses', 'B_win_by_Submission', 'Blue - DEC', 'B_draw', 'R_Middleweight_rank', 'B_avg_SIG_STR_pct', \"B_Women's Strawweight_rank\", 'age_dif', 'B_Middleweight_rank', 'avg_sub_att_dif', 'avg_td_dif', 'B_avg_SUB_ATT', \"R_Women's Flyweight_rank\", 'R_Pound-for-Pound_rank', 'R_Reach_cms', 'R_wins', 'B_ev', 'R_Weight_lbs', 'sub_dif', 'R_ev', 'B_avg_TD_landed']\n",
      "0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2782, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2782, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250     0\n",
      "252     5\n",
      "253     2\n",
      "254     3\n",
      "255     0\n",
      "       ..\n",
      "4162    0\n",
      "4170    1\n",
      "4171    3\n",
      "4172    5\n",
      "4173    5\n",
      "Name: label, Length: 2782, dtype: int64\n",
      "Real Score: 6180.0\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n",
      "HI\n"
     ]
    }
   ],
   "source": [
    "keep_going = True\n",
    "#keep_going = False\n",
    "while(keep_going):\n",
    "\n",
    "    \n",
    "    #1. Set Features\n",
    "    #get_best_features(pos_features, m, df, cur_features, labels, odds, scale=False)\n",
    "    test_model_features = (get_best_features_mov(my_pos_features, test_model, df_train, test_model_features, label_train, odds_train, finish_list, \n",
    "                                             min_ev=test_model_ev))\n",
    "    print_model()\n",
    "    save_model()\n",
    "    #2 Set hyperparameters\n",
    "    #def tune_hyperparameters(input_model, input_features, input_df, input_labels, odds_input):\n",
    "    test_model = tune_hyperparameters(test_model, test_model_features, df_train, label_train, odds_train, \n",
    "                                      min_ev=test_model_ev)\n",
    "    \n",
    "    print_model()\n",
    "    save_model()    \n",
    "    \n",
    "    #3. Set EV\n",
    "    #def tune_ev(input_model, input_features, input_df, input_labels, odds_input, verbose=False):\n",
    "    test_model_ev = tune_ev_mov(test_model, test_model_features, df_train, label_train, odds_train, verbose=False)\n",
    "    old_test_model_features = test_model_features #This prevents\n",
    "                                                  #an uneccesary loop\n",
    "    print_model()\n",
    "    save_model()\n",
    "    \n",
    "    \n",
    "    #4. Remove Features\n",
    "    #def remove_to_improve(cur_features, m, df, labels, odds, scale=False, min_ev = 0):\n",
    "    test_model_features = remove_to_improve_mov(test_model_features, test_model, df_train, label_train, odds_train, min_ev = test_model_ev)    \n",
    "    keep_going = False\n",
    "    \n",
    "    print_model()\n",
    "    save_model()\n",
    "    if old_test_model != test_model:\n",
    "        print(\"The hyperparameters are different\")\n",
    "        print(\"OLD:\")\n",
    "        print(old_test_model)\n",
    "        print(\"NEW:\")\n",
    "        print(test_model)\n",
    "        keep_going = True\n",
    "        old_test_model = test_model\n",
    "    if old_test_model_features != test_model_features:\n",
    "        print(\"The features are different\")\n",
    "        print(\"OLD:\")\n",
    "        print(old_test_model_features)\n",
    "        print(\"NEW:\")\n",
    "        print(test_model_features)\n",
    "        keep_going = True\n",
    "        old_test_model_features = test_model_features\n",
    "    if old_test_model_ev != test_model_ev:\n",
    "        print(\"The EV is different\")\n",
    "        print(\"OLD:\")\n",
    "        print(old_test_model_ev)\n",
    "        print(\"NEW:\")\n",
    "        print(test_model_ev)\n",
    "        keep_going = True\n",
    "        old_test_model_ev = test_model_ev\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(test_model_name) \n",
    "print(models[0][model_num])\n",
    "print()\n",
    "print(test_model)\n",
    "print(eval(models[1][model_num]))\n",
    "print()\n",
    "print(test_model_features) \n",
    "print(eval(models[2][model_num]))\n",
    "print()\n",
    "print(test_model_ev)\n",
    "print(eval(models[3][model_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
