{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8d589d",
   "metadata": {},
   "source": [
    "# What we are going to do here is get model scores for Method of Victory models (mov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0966027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "#from sklearn.mixture import DPGMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a85dcfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../automated_model_creation') #We need to access the function file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5db8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45d95c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_prod = True\n",
    "\n",
    "if is_prod:\n",
    "    model_file = '../../data/production_models_mov.csv'\n",
    "else: #If not production then test\n",
    "    model_file = '../../data/models_mov.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29cf82bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "#Load models\n",
    "with open(model_file, newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    models = list(reader)\n",
    "    \n",
    "print(len(models[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45efde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/production_models_mov.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "081538d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/kaggle_data/ufc-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81cfcf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83675f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_finish_type(winner, finish):\n",
    "    #print(winner, finish)\n",
    "    #Why overcomplicate things?  We can just use a few if statements\n",
    "    if winner == 'Red':\n",
    "        #print(\"HI\")\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Red - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Red - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Red - KO/TKO')\n",
    "    if winner == 'Blue':\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Blue - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Blue - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Blue - KO/TKO')\n",
    "        \n",
    "    #Test for NaN\n",
    "    if finish != finish:\n",
    "        return('')\n",
    "    \n",
    "    if finish == 'Overturned':\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "    return ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99c533c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calls for the power of lambda!\n",
    "df['finish_type'] = df.apply(lambda x: return_finish_type(x['Winner'], x['finish']), axis=1)\n",
    "mask = df['finish_type'] != ''\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5350bf60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-af8f7542de83>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = f\n"
     ]
    }
   ],
   "source": [
    "finish_list = ['Red - DEC', 'Red - SUB', 'Red - KO/TKO', 'Blue - DEC', 'Blue - SUB', 'Blue - KO/TKO']\n",
    "\n",
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "\n",
    "for f in range(len(finish_list)):\n",
    "    mask = df['finish_type'] == finish_list[f]\n",
    "    df['label'][mask] = f\n",
    "    \n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ef17c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'r_dec_odds': 'Red - DEC', 'r_sub_odds': 'Red - SUB', 'r_ko_odds': 'Red - KO/TKO',\n",
    "                'b_dec_odds': 'Blue - DEC', 'b_sub_odds': 'Blue - SUB', 'b_ko_odds': 'Blue - KO/TKO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4aac584",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df['label']\n",
    "odds_df = df[finish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a464bf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n",
    "#as we pick models\n",
    "\n",
    "df_train = df[250:]\n",
    "odds_train = odds_df[250:]\n",
    "label_train = label_df[250:]\n",
    "\n",
    "df_test = df[:250]\n",
    "odds_test = odds_df[:250]\n",
    "label_test = label_df[:250]\n",
    "\n",
    "#print(len(df_test))\n",
    "#print(len(odds_test))\n",
    "#print(len(label_test))\n",
    "\n",
    "#print(len(df_train))\n",
    "#print(len(odds_train))\n",
    "#print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eb13d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to clean\n",
    "mask = df_train['finish_type'] != ''\n",
    "df_train = df_train[mask]\n",
    "#print(len(df_train))\n",
    "\n",
    "mask = df_test['finish_type'] != ''\n",
    "df_test = df_test[mask]\n",
    "#print(len(df_test))\n",
    "\n",
    "label_train = label_train[label_train.index.isin(df_train.index)]\n",
    "label_test = label_test[label_test.index.isin(df_test.index)]\n",
    "\n",
    "odds_train = odds_train[odds_train.index.isin(df_train.index)]\n",
    "odds_test = odds_test[odds_test.index.isin(df_test.index)]\n",
    "\n",
    "\n",
    "#print(len(df_train))\n",
    "#print(len(label_train))\n",
    "#print(len(odds_train))\n",
    "#print(len(df_test))\n",
    "#print(len(label_test))\n",
    "#print(len(odds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c8e1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a value for the nulls in the ranks\n",
    "\n",
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d579569f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74351ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf53315",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f283a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2deed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d380c761",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5969cafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2867, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2867,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2867, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242, 26)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(242, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250     5\n",
      "251     2\n",
      "252     5\n",
      "253     3\n",
      "254     0\n",
      "       ..\n",
      "4251    0\n",
      "4259    1\n",
      "4260    3\n",
      "4261    5\n",
      "4262    5\n",
      "Name: label, Length: 2867, dtype: int64\n",
      "Real Score: 10235.0\n"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "for model_num in range(len(models[0])):\n",
    "    #print(test_model_features)\n",
    "    #print(len())\n",
    "    test_model_name = models[0][model_num]\n",
    "    test_model = eval(models[1][model_num])\n",
    "    test_model_features = eval(models[2][model_num])\n",
    "    test_model_ev = eval(models[3][model_num])\n",
    "    score_list.append(evaluate_model_mov(test_model, test_model_features, test_model_ev, df_train, label_train, odds_train, df_test, label_test,\n",
    "                         odds_test, finish_list, verbose = True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "670d7edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[4] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4eae1117",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_file, 'w' , newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    for row in models:\n",
    "        #print(\"HI\")\n",
    "        writer.writerow(row)\n",
    "    \n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "905df7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10235.0]\n"
     ]
    }
   ],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cb40b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[4] = score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "044637ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10235.0]\n"
     ]
    }
   ],
   "source": [
    "print(score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8e7eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
