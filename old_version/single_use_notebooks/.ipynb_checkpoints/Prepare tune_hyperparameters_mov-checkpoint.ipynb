{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85b64658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d49346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/kaggle_data/ufc-master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88bc2dbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4783"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ef602bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's fix the date\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba55a3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_finish_type(winner, finish):\n",
    "    #print(winner, finish)\n",
    "    #Why overcomplicate things?  We can just use a few if statements\n",
    "    if winner == 'Red':\n",
    "        #print(\"HI\")\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Red - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Red - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Red - KO/TKO')\n",
    "    if winner == 'Blue':\n",
    "        if finish in ['U-DEC', 'S-DEC', 'M-DEC']:\n",
    "            return ('Blue - DEC')\n",
    "        if finish in ['SUB']:\n",
    "            return('Blue - SUB')\n",
    "        if finish in ['KO/TKO', 'DQ']:\n",
    "            return('Blue - KO/TKO')\n",
    "        \n",
    "    #Test for NaN\n",
    "    if finish != finish:\n",
    "        return('')\n",
    "    \n",
    "    if finish == 'Overturned':\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "    return ('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ba1f75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This calls for the power of lambda!\n",
    "df['finish_type'] = df.apply(lambda x: return_finish_type(x['Winner'], x['finish']), axis=1)\n",
    "mask = df['finish_type'] != ''\n",
    "df = df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ca53cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-af8f7542de83>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['label'][mask] = f\n"
     ]
    }
   ],
   "source": [
    "finish_list = ['Red - DEC', 'Red - SUB', 'Red - KO/TKO', 'Blue - DEC', 'Blue - SUB', 'Blue - KO/TKO']\n",
    "\n",
    "#Let's put all the labels in a dataframe\n",
    "df['label'] = ''\n",
    "#If the winner is not Red or Blue we can remove it.\n",
    "\n",
    "for f in range(len(finish_list)):\n",
    "    mask = df['finish_type'] == finish_list[f]\n",
    "    df['label'][mask] = f\n",
    "    \n",
    "#df[\"Winner\"] = df[\"Winner\"].astype('category')\n",
    "#df = df[(df['Winner'] != 'Blue') | (df['Winner'] == 'Red') ]\n",
    "\n",
    "\n",
    "#Make sure lable is numeric\n",
    "df['label'] = pd.to_numeric(df['label'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b29698ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'r_dec_odds': 'Red - DEC', 'r_sub_odds': 'Red - SUB', 'r_ko_odds': 'Red - KO/TKO',\n",
    "                'b_dec_odds': 'Blue - DEC', 'b_sub_odds': 'Blue - SUB', 'b_ko_odds': 'Blue - KO/TKO'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30884c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df['label']\n",
    "odds_df = df[finish_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b13836ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250\n",
      "250\n",
      "250\n",
      "4293\n",
      "4293\n",
      "4293\n"
     ]
    }
   ],
   "source": [
    "#Split the test set.  We are always(?) going to use the last 200 matches as the test set, so we don't want those around\n",
    "#as we pick models\n",
    "\n",
    "df_train = df[250:]\n",
    "odds_train = odds_df[250:]\n",
    "label_train = label_df[250:]\n",
    "\n",
    "df_test = df[:250]\n",
    "odds_test = odds_df[:250]\n",
    "label_test = label_df[:250]\n",
    "\n",
    "print(len(df_test))\n",
    "print(len(odds_test))\n",
    "print(len(label_test))\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(odds_train))\n",
    "print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c7b904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4293\n",
      "4293\n",
      "4293\n",
      "250\n",
      "250\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "#We need to clean\n",
    "mask = df_train['finish_type'] != ''\n",
    "df_train = df_train[mask]\n",
    "#print(len(df_train))\n",
    "\n",
    "mask = df_test['finish_type'] != ''\n",
    "df_test = df_test[mask]\n",
    "#print(len(df_test))\n",
    "\n",
    "label_train = label_train[label_train.index.isin(df_train.index)]\n",
    "label_test = label_test[label_test.index.isin(df_test.index)]\n",
    "\n",
    "odds_train = odds_train[odds_train.index.isin(df_train.index)]\n",
    "odds_test = odds_test[odds_test.index.isin(df_test.index)]\n",
    "\n",
    "\n",
    "print(len(df_train))\n",
    "print(len(label_train))\n",
    "print(len(odds_train))\n",
    "print(len(df_test))\n",
    "print(len(label_test))\n",
    "print(len(odds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e503be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weightclass_list = ['B_match_weightclass_rank', 'R_match_weightclass_rank', \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\", \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', 'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', 'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", 'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', 'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', 'B_Pound-for-Pound_rank']\n",
    "df_train[weightclass_list] = df_train[weightclass_list].fillna(17)\n",
    "df_test[weightclass_list] = df_test[weightclass_list].fillna(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "35a924c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_name = \"dummy_model_for_trace\"\n",
    "test_model = DecisionTreeClassifier(random_state=75)\n",
    "test_model_features = ['R_odds']\n",
    "test_model_ev = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2b336f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_test_model = test_model\n",
    "old_test_model_features = test_model_features\n",
    "old_test_model_ev = test_model_ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7635f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pos_features = ['R_odds', 'B_odds', 'R_ev', 'B_ev',\n",
    "       'location', 'country', 'title_bout', 'weight_class', 'gender',\n",
    "       'no_of_rounds', 'B_current_lose_streak', 'B_current_win_streak',\n",
    "       'B_draw', 'B_avg_SIG_STR_landed', 'B_avg_SIG_STR_pct', 'B_avg_SUB_ATT',\n",
    "       'B_avg_TD_landed', 'B_avg_TD_pct', 'B_longest_win_streak', 'B_losses',\n",
    "       'B_total_rounds_fought', 'B_total_title_bouts',\n",
    "       'B_win_by_Decision_Majority', 'B_win_by_Decision_Split',\n",
    "       'B_win_by_Decision_Unanimous', 'B_win_by_KO/TKO', 'B_win_by_Submission',\n",
    "       'B_win_by_TKO_Doctor_Stoppage', 'B_wins', 'B_Stance', 'B_Height_cms',\n",
    "       'B_Reach_cms', 'B_Weight_lbs', 'R_current_lose_streak',\n",
    "       'R_current_win_streak', 'R_draw', 'R_avg_SIG_STR_landed',\n",
    "       'R_avg_SIG_STR_pct', 'R_avg_SUB_ATT', 'R_avg_TD_landed', 'R_avg_TD_pct',\n",
    "       'R_longest_win_streak', 'R_losses', 'R_total_rounds_fought',\n",
    "       'R_total_title_bouts', 'R_win_by_Decision_Majority',\n",
    "       'R_win_by_Decision_Split', 'R_win_by_Decision_Unanimous',\n",
    "       'R_win_by_KO/TKO', 'R_win_by_Submission',\n",
    "       'R_win_by_TKO_Doctor_Stoppage', 'R_wins', 'R_Stance', 'R_Height_cms',\n",
    "       'R_Reach_cms', 'R_Weight_lbs', 'R_age', 'B_age', 'lose_streak_dif',\n",
    "       'win_streak_dif', 'longest_win_streak_dif', 'win_dif', 'loss_dif',\n",
    "       'total_round_dif', 'total_title_bout_dif', 'ko_dif', 'sub_dif',\n",
    "       'height_dif', 'reach_dif', 'age_dif', 'sig_str_dif', 'avg_sub_att_dif',\n",
    "       'avg_td_dif', 'empty_arena', 'B_match_weightclass_rank', 'R_match_weightclass_rank', \n",
    "        \"R_Women's Flyweight_rank\", \"R_Women's Featherweight_rank\", \"R_Women's Strawweight_rank\",\n",
    "        \"R_Women's Bantamweight_rank\", 'R_Heavyweight_rank', 'R_Light Heavyweight_rank', \n",
    "        'R_Middleweight_rank', 'R_Welterweight_rank', 'R_Lightweight_rank', 'R_Featherweight_rank', \n",
    "        'R_Bantamweight_rank', 'R_Flyweight_rank', 'R_Pound-for-Pound_rank', \"B_Women's Flyweight_rank\", \n",
    "        \"B_Women's Featherweight_rank\", \"B_Women's Strawweight_rank\", \"B_Women's Bantamweight_rank\", \n",
    "        'B_Heavyweight_rank', 'B_Light Heavyweight_rank', 'B_Middleweight_rank', 'B_Welterweight_rank', \n",
    "        'B_Lightweight_rank', 'B_Featherweight_rank', 'B_Bantamweight_rank', 'B_Flyweight_rank', \n",
    "        'B_Pound-for-Pound_rank', 'Red - DEC', 'Blue - DEC', 'Red - SUB', 'Blue - SUB', 'Red - KO/TKO', 'Blue - KO/TKO', 'better_rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2427a24c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy_model_for_trace\n",
      "DecisionTreeClassifier(random_state=75)\n",
      "['R_odds']\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(test_model_name)\n",
    "print(test_model)\n",
    "print(test_model_features)\n",
    "print(test_model_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d768696",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model():\n",
    "    print()\n",
    "    print(test_model_name)\n",
    "    print(test_model)\n",
    "    print(test_model_features)\n",
    "    print(test_model_ev)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71af71e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_hyperparameters_mov' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-33b23f2f7589>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m test_model = tune_hyperparameters_mov(test_model, test_model_features, df_train, label_train, odds_train, \n\u001b[0m\u001b[0;32m      2\u001b[0m                                       min_ev=test_model_ev)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tune_hyperparameters_mov' is not defined"
     ]
    }
   ],
   "source": [
    "test_model = tune_hyperparameters_mov(test_model, test_model_features, df_train, label_train, odds_train, \n",
    "                                      min_ev=test_model_ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd888f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
